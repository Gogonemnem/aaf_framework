{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Function to read JSON file and parse results\n",
    "def read_results(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    return results\n",
    "\n",
    "# Function to compute mean metrics over the classes\n",
    "def compute_mean_metrics(results):\n",
    "    analysis = {}\n",
    "    for config, data in results.items():\n",
    "        few_shot_data = data.get(\"few_shot\", {})\n",
    "        analysis[config] = {}\n",
    "        for shot, shot_data in few_shot_data.items():\n",
    "            analysis[config][shot] = {\"train\": {}, \"test\": {}}\n",
    "            for res_type in [\"train\", \"test\"]:\n",
    "                res_data = shot_data.get(res_type, {})\n",
    "                overall_metrics = res_data.get(\"overall\", {})\n",
    "                class_metrics = {k: v for k, v in res_data.items() if k != \"overall\"}\n",
    "                \n",
    "                # Dynamically get the metric names from the class metrics\n",
    "                if class_metrics:\n",
    "                    sample_class = next(iter(class_metrics.values()))\n",
    "                    metric_names = sample_class.keys()\n",
    "                else:\n",
    "                    metric_names = []\n",
    "                \n",
    "                # Initialize dictionary to hold the sums and counts for each metric\n",
    "                metric_sums = {metric: 0 for metric in metric_names}\n",
    "                metric_counts = {metric: 0 for metric in metric_names}\n",
    "                \n",
    "                # Collect metric values for each class\n",
    "                for metrics in class_metrics.values():\n",
    "                    for metric in metric_names:\n",
    "                        value = metrics[metric]\n",
    "                        if metric in metrics and not np.isnan(value) and value != -1:\n",
    "                            metric_sums[metric] += value\n",
    "                            metric_counts[metric] += 1\n",
    "                \n",
    "                # Compute mean for each metric\n",
    "                mean_metrics = {metric: (metric_sums[metric] / metric_counts[metric] if metric_counts[metric] > 0 else float('nan')) for metric in metric_names}\n",
    "                analysis[config][shot][res_type] = {\n",
    "                    \"mean_metrics\": mean_metrics,\n",
    "                    \"overall\": overall_metrics\n",
    "                }\n",
    "                \n",
    "    return analysis\n",
    "\n",
    "# Function to print analysis results\n",
    "def print_analysis(analysis):\n",
    "    for config, data in analysis.items():\n",
    "        print(f\"Config: {config}\")\n",
    "        for shot, shot_data in data.items():\n",
    "            print(f\"  Shot: {shot}\")\n",
    "            for res_type, res_data in shot_data.items():\n",
    "                mean_metrics = res_data[\"mean_metrics\"]\n",
    "                overall = res_data[\"overall\"]\n",
    "                print(f\"    {res_type.capitalize()}:\")\n",
    "                for metric, value in mean_metrics.items():\n",
    "                    print(f\"      Mean {metric}: {value:.4f}\")\n",
    "                print(f\"      Overall Metrics: {overall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fcos_PVT_V2_B2_LI_FPN_RETINANET_DOTA.yaml': {'1_shot': {'train': {'mean_metrics': {'AP': 0.22985315794179295, 'AP50': 0.46208595592760915, 'AP75': 0.19618684590283667, 'APs': 0.11526205081481823, 'APm': 0.25981433616809163, 'APl': 0.32909304642691833}, 'overall': {'AP': 0.23188080445577552, 'AP50': 0.42217647024369953, 'AP75': 0.2302174036273389, 'APs': 0.29920536498061295, 'APm': 0.23801598296124424, 'APl': 0.3210372678343144}}, 'test': {'mean_metrics': {'AP': 0.08653326682288144, 'AP50': 0.17050578125707072, 'AP75': 0.08345988111254893, 'APs': 0.06436167150106861, 'APm': 0.10191601176728643, 'APl': 0.11975179410020494}, 'overall': {'AP': 0.08653326682288145, 'AP50': 0.17050578125707072, 'AP75': 0.08345988111254893, 'APs': 0.06436167150106861, 'APm': 0.10191601176728643, 'APl': 0.11975179410020495}}}, '10_shot': {'train': {'mean_metrics': {'AP': 0.26674808952491075, 'AP50': 0.5374720466719125, 'AP75': 0.2319565906691231, 'APs': 0.1281714372653335, 'APm': 0.30015004094276865, 'APl': 0.35265423681105973}, 'overall': {'AP': 0.22942493801890648, 'AP50': 0.4217879506417914, 'AP75': 0.2256436564284472, 'APs': 0.1855704981354152, 'APm': 0.23081527080151498, 'APl': 0.3091329720546915}}, 'test': {'mean_metrics': {'AP': 0.22866288221883405, 'AP50': 0.36591057213818606, 'AP75': 0.25632911260795455, 'APs': 0.1764129147810379, 'APm': 0.2300251797318771, 'APl': 0.2735344675345593}, 'overall': {'AP': 0.22866288221883407, 'AP50': 0.365910572138186, 'AP75': 0.2563291126079546, 'APs': 0.1764129147810379, 'APm': 0.23002517973187714, 'APl': 0.27353446753455923}}}}}\n",
      "Config: fcos_PVT_V2_B2_LI_FPN_RETINANET_DOTA.yaml\n",
      "  Shot: 1_shot\n",
      "    Train:\n",
      "      Mean AP: 0.2299\n",
      "      Mean AP50: 0.4621\n",
      "      Mean AP75: 0.1962\n",
      "      Mean APs: 0.1153\n",
      "      Mean APm: 0.2598\n",
      "      Mean APl: 0.3291\n",
      "      Overall Metrics: {'AP': 0.23188080445577552, 'AP50': 0.42217647024369953, 'AP75': 0.2302174036273389, 'APs': 0.29920536498061295, 'APm': 0.23801598296124424, 'APl': 0.3210372678343144}\n",
      "    Test:\n",
      "      Mean AP: 0.0865\n",
      "      Mean AP50: 0.1705\n",
      "      Mean AP75: 0.0835\n",
      "      Mean APs: 0.0644\n",
      "      Mean APm: 0.1019\n",
      "      Mean APl: 0.1198\n",
      "      Overall Metrics: {'AP': 0.08653326682288145, 'AP50': 0.17050578125707072, 'AP75': 0.08345988111254893, 'APs': 0.06436167150106861, 'APm': 0.10191601176728643, 'APl': 0.11975179410020495}\n",
      "  Shot: 10_shot\n",
      "    Train:\n",
      "      Mean AP: 0.2667\n",
      "      Mean AP50: 0.5375\n",
      "      Mean AP75: 0.2320\n",
      "      Mean APs: 0.1282\n",
      "      Mean APm: 0.3002\n",
      "      Mean APl: 0.3527\n",
      "      Overall Metrics: {'AP': 0.22942493801890648, 'AP50': 0.4217879506417914, 'AP75': 0.2256436564284472, 'APs': 0.1855704981354152, 'APm': 0.23081527080151498, 'APl': 0.3091329720546915}\n",
      "    Test:\n",
      "      Mean AP: 0.2287\n",
      "      Mean AP50: 0.3659\n",
      "      Mean AP75: 0.2563\n",
      "      Mean APs: 0.1764\n",
      "      Mean APm: 0.2300\n",
      "      Mean APl: 0.2735\n",
      "      Overall Metrics: {'AP': 0.22866288221883407, 'AP50': 0.365910572138186, 'AP75': 0.2563291126079546, 'APs': 0.1764129147810379, 'APm': 0.23002517973187714, 'APl': 0.27353446753455923}\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSON results file\n",
    "file_path = 'evaluation_results.json'\n",
    "\n",
    "# Read the results from the JSON file\n",
    "results = read_results(file_path)\n",
    "\n",
    "# Compute the analysis\n",
    "analysis = compute_mean_metrics(results)\n",
    "print(analysis)\n",
    "# Print the analysis\n",
    "print_analysis(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
