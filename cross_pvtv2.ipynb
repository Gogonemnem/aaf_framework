{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.pvt_v2 import Attention\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, module, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for attr_name, attr_value in module.__dict__.items():\n",
    "            setattr(self, attr_name, attr_value)\n",
    "\n",
    "    def forward(self, \n",
    "                x_query, feat_size_query: List[int],\n",
    "                x_support, feat_size_support: List[int]):\n",
    "\n",
    "        # B, N, C = x.shape\n",
    "        # H, W = feat_size\n",
    "        if x_query.shape[0] == 1:\n",
    "            xs = [x_query, x_support]\n",
    "            feat_sizes = [feat_size_query, feat_size_support]\n",
    "        elif x_support.shape[0] == 1:\n",
    "            xs = [x_support, x_query]\n",
    "            feat_sizes = [feat_size_support, feat_size_query]\n",
    "        else:\n",
    "            raise ValueError('Either the query or support tensor should have a batch size of 1')\n",
    "\n",
    "        qs = [self.q(x).reshape(*x.shape[:2], self.num_heads, -1).permute(0, 2, 1, 3) for x in xs]\n",
    "\n",
    "        ks, vs = [], []\n",
    "        for x, feat_size in zip(xs, feat_sizes):\n",
    "            B, N, C = x.shape\n",
    "            H, W = feat_size\n",
    "\n",
    "            if self.pool is not None:\n",
    "                x = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "                x = self.sr(self.pool(x)).reshape(B, C, -1).permute(0, 2, 1)\n",
    "                x = self.norm(x)\n",
    "                x = self.act(x)\n",
    "            elif self.sr is not None:\n",
    "                x = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "                x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1)\n",
    "                x = self.norm(x)\n",
    "            \n",
    "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)  \n",
    "            ks.append(kv[0]), vs.append(kv[1])\n",
    "\n",
    "        k_expanded = ks[0].expand(ks[1].shape[0], -1, -1, -1)\n",
    "        k_mean = ks[1].mean(dim=0, keepdim=True)\n",
    "\n",
    "        v_expanded = vs[0].expand(vs[1].shape[0], -1, -1, -1)\n",
    "        v_mean = vs[1].mean(dim=0, keepdim=True)\n",
    "\n",
    "        k_cats = [torch.cat((ks[0], k_mean), dim=2), torch.cat((ks[1], k_expanded), dim=2)]\n",
    "        v_cats = [torch.cat((vs[0], v_mean), dim=2), torch.cat((vs[1], v_expanded), dim=2)]\n",
    "\n",
    "        outputs = []\n",
    "        for x, feat_size, q, k_cat, v_cat in zip(xs, feat_sizes, qs, k_cats, v_cats):\n",
    "            B, N, C = x.shape\n",
    "            H, W = feat_size\n",
    "            \n",
    "\n",
    "            if self.fused_attn:\n",
    "                x = F.scaled_dot_product_attention(q, k_cat, v_cat, dropout_p=self.attn_drop.p if self.training else 0.)\n",
    "            else:\n",
    "                q = q * self.scale\n",
    "                attn = q @ k_cat.transpose(-2, -1)\n",
    "                attn = attn.softmax(dim=-1)\n",
    "                attn = self.attn_drop(attn)\n",
    "                x = attn @ v_cat\n",
    "\n",
    "            x = x.transpose(1, 2).reshape(B, N, C)\n",
    "            x = self.proj(x)\n",
    "            x = self.proj_drop(x)\n",
    "            outputs.append(x)\n",
    "        \n",
    "        if x_support.shape[0] == 1:\n",
    "            return reversed(outputs)\n",
    "        return tuple(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "class CrossPyramidVisionTransformerV2(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Replace Attention layers with CrossAttention layers\n",
    "        self.replace_attention_layers(model, Attention, CrossAttention)\n",
    "\n",
    "        self.model = model\n",
    "        for attr_name, attr_value in model.__dict__.items():\n",
    "            setattr(self, attr_name, attr_value)\n",
    "    \n",
    "    def replace_attention_layers(self, module, target_class, replace_class):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, target_class):\n",
    "                new_layer = replace_class(child)\n",
    "                setattr(module, name, new_layer)\n",
    "            else:\n",
    "                self.replace_attention_layers(child, target_class, replace_class)\n",
    "\n",
    "    def forward_features(self, query, support):\n",
    "        x_query = self.patch_embed(query)\n",
    "        x_support = self.patch_embed(support)\n",
    "\n",
    "        for stage in self.stages:\n",
    "            if stage.downsample is not None:\n",
    "                x_query = stage.downsample(x_query)\n",
    "                x_support = stage.downsample(x_support)\n",
    "            \n",
    "            B_q, H_q, W_q, C_q = x_query.shape\n",
    "            feat_size_query = (H_q, W_q)\n",
    "            x_query = x_query.reshape(B_q, -1, C_q)\n",
    "\n",
    "            B_s, H_s, W_s, C_s = x_support.shape\n",
    "            feat_size_support = (H_s, W_s)\n",
    "            x_support = x_support.reshape(B_s, -1, C_s)\n",
    "\n",
    "            for block in stage.blocks:\n",
    "                if isinstance(block.attn, CrossAttention):\n",
    "                    x_query, x_support = block.attn(block.norm1(x_query), feat_size_query, block.norm1(x_support), feat_size_support)\n",
    "\n",
    "                    x_query = x_query + block.drop_path1(x_query)\n",
    "                    x_query = x_query + block.drop_path2(block.mlp(block.norm2(x_query), feat_size_query))\n",
    "\n",
    "                    x_support = x_support + block.drop_path1(x_support)\n",
    "                    x_support = x_support + block.drop_path2(block.mlp(block.norm2(x_support), feat_size_support))\n",
    "                else:\n",
    "                    x_query = block(x_query, feat_size_query)\n",
    "                    x_support = block(x_support, feat_size_support)\n",
    "            \n",
    "            x_query = stage.norm(x_query)\n",
    "            x_support = stage.norm(x_support)\n",
    "\n",
    "            x_query = x_query.reshape(B_q, feat_size_query[0], feat_size_query[1], -1).permute(0, 3, 1, 2).contiguous()\n",
    "            x_support = x_support.reshape(B_s, feat_size_support[0], feat_size_support[1], -1).permute(0, 3, 1, 2).contiguous()\n",
    "        return x_query, x_support\n",
    "\n",
    "    def forward(self, query, support):\n",
    "        x_query, x_support = self.forward_features(query, support)\n",
    "        x = self.head_drop(x_query.mean(dim=[2, 3])) if self.global_pool else x_query\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# Load the pre-trained PVTv2 model from TIMM\n",
    "from timm import create_model\n",
    "\n",
    "model = create_model('pvt_v2_b0', pretrained=True)\n",
    "cross_pvtv2 = CrossPyramidVisionTransformerV2(model)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = torch.randn(10, 3, 128, 128)\n",
    "support = torch.randn(1, 3, 80, 80)\n",
    "output = cross_pvtv2(query, support)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
